import os
import json
from pathlib import Path
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
from functools import partial
from typing import List

# è®¾ç½®æ ¹ç›®å½•å’Œ JSON æ–‡ä»¶å
ROOT_DIR = Path("/home_sfs/zhouenshen/dataset/3D/cubifyanything/filter_step_20")
JSON_NAME = "detection_GroundingDino_bbox_RAM_label_qwen_caption_confidence25.json"

def find_jsons_in_video_dir(video_dir: Path, json_name: str) -> List[Path]:
    """åœ¨ä¸€ä¸ªè§†é¢‘ç›®å½•ä¸‹æŸ¥æ‰¾æ‰€æœ‰ wide/xxx.json æ–‡ä»¶"""
    results = []
    try:
        for frame_dir in video_dir.iterdir():
            json_path = frame_dir / "wide" / json_name
            if json_path.exists():
                results.append(json_path)
    except Exception as e:
        print(f"âŒ Error in {video_dir}: {e}")
    return results

def find_all_json_files_parallel(root_dir: Path, json_name: str, num_workers: int = None) -> List[Path]:
    """å¤šè¿›ç¨‹æŸ¥æ‰¾æ‰€æœ‰å¸§ç›®å½•ä¸­çš„ç›®æ ‡ JSON æ–‡ä»¶"""
    video_dirs = [d for d in root_dir.iterdir() if d.is_dir()]

    if num_workers is None:
        num_workers = max(1, cpu_count())

    print(f"ğŸ” Found {len(video_dirs)} video dirs. Using {num_workers} workers to search JSON files...")
    json_paths = []
    with Pool(processes=num_workers) as pool:
        func = partial(find_jsons_in_video_dir, json_name=json_name)
        for json_list in tqdm(pool.imap_unordered(func, video_dirs), total=len(video_dirs), desc="Scanning"):
            json_paths.extend(json_list)
    print(f"âœ… Total JSON files found: {len(json_paths)}")
    return json_paths

def resize_bbox(bbox):
    """å°† bbox ç¼©å°ä¸ºåŸæ¥çš„ 1/2"""
    return [coord / 2 for coord in bbox]

def process_single_json(json_path: Path, overwrite=True):
    try:
        with open(json_path, "r") as f:
            data = json.load(f)

        # æ£€æŸ¥ objects
        if "objects" not in data or not isinstance(data["objects"], list):
            print("âŒ No 'objects' found in the JSON file.")
            return False

        # ä¿®æ”¹æ¯ä¸ª object çš„ bbox
        for obj in data["objects"]:
            if "xyxy" in obj and isinstance(obj["xyxy"], list) and len(obj["xyxy"]) == 4:
                obj["bbox_resized"] = resize_bbox(obj["xyxy"])
            else:
                print(f"âŒ Invalid bbox format in {json_path}")

        # å†™å›æ–‡ä»¶
        if overwrite:
            output_path = json_path
        else:
            output_path = json_path.with_name(json_path.stem + "_resized.json")

        with open(output_path, "w") as f:
            json.dump(data, f, indent=2)

        return True

    except Exception as e:
        print(f"âŒ Error processing {json_path}: {e}")
        return False

def main():
    json_paths = find_all_json_files_parallel(ROOT_DIR, JSON_NAME)
    print(f"ğŸ” æ‰¾åˆ° {len(json_paths)} ä¸ª JSON æ–‡ä»¶ï¼Œå¼€å§‹å¤šè¿›ç¨‹å¤„ç†...")

    with Pool(cpu_count()) as pool:
        results = list(tqdm(pool.imap_unordered(process_single_json, json_paths), total=len(json_paths)))

    success = sum(results)
    print(f"âœ… å®Œæˆå¤„ç†ï¼šæˆåŠŸ {success}ï¼Œå¤±è´¥ {len(results) - success}")

if __name__ == "__main__":
    main()