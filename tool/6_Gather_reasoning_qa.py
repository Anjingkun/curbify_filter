#!/usr/bin/env python3
import json
from pathlib import Path
from tqdm import tqdm
from multiprocessing import Pool, cpu_count

# âœ… ä¿®æ”¹ä¸ºä½ çš„æ ¹ç›®å½•
ROOT_DIR = "/home_sfs/zhouenshen/dataset/3D/cubifyanything/llm_qa"

def find_all_frame_json_files(root_path: str) -> list[Path]:
    """
    æŸ¥æ‰¾ root_path ä¸‹æ‰€æœ‰ç¬¦åˆä»¥ä¸‹ç›®å½•ç»“æ„çš„ JSON æ–‡ä»¶ï¼š
      ROOT_DIR/æ•°å­—æ–‡ä»¶å¤¹/video_id/frame_id.json
    ä½¿ç”¨ glob æ¨¡å¼ï¼š "*/*/*.json"
    è¿”å›æ‰€æœ‰ JSON æ–‡ä»¶çš„å®Œæ•´è·¯å¾„åˆ—è¡¨ï¼ˆPath å¯¹è±¡ï¼‰ã€‚
    """
    root = Path(root_path).resolve()
    if not root.exists():
        raise FileNotFoundError(f"Root path does not exist: {root}")
    json_files = list(root.glob("*/*/*.json"))
    print(f"ğŸ” Found {len(json_files)} json files under {root}")
    return json_files

def process_frame_json(json_file: Path) -> dict | None:
    """
    å¤„ç†å•ä¸ª JSON æ–‡ä»¶ï¼Œè½¬æ¢ä¸ºç›®æ ‡æ•°æ®æ ¼å¼ï¼š
      - id: f"{video_id}_{frame_id}"ï¼Œå…¶ä¸­ video_id ä¸ºçˆ¶æ–‡ä»¶å¤¹åç§°ï¼Œframe_id ä¸ºæ–‡ä»¶åå»æ‰ .json åç¼€ã€‚
      - image: [ f"{video_id}{frame_id}_wide_image.png" ]
      - depth: [ f"{video_id}{frame_id}_wide_depth.png" ]
      - conversations: å¯¹åŸ conversations ä¸­æ¯ä¸ªå¯¹è¯æ¡ç›®ï¼Œ
          æŒ‰é¡ºåºç”Ÿæˆä¸¤æ¡æ¶ˆæ¯ï¼Œä¸€æ¡ humanï¼ˆå– question å­—æ®µï¼‰ï¼Œä¸€æ¡ gptï¼ˆå– answer å­—æ®µï¼‰ã€‚
    """
    try:
        # è·å– video_id å’Œ frame_idï¼ˆæ³¨æ„ï¼šè¿™é‡Œ video_id æ¥è‡ª JSON æ–‡ä»¶æ‰€åœ¨çš„è§†é¢‘æ–‡ä»¶å¤¹åç§°ï¼‰
        video_id = json_file.parent.name
        frame_id = json_file.stem  # æ–‡ä»¶åå»æ‰ .json åç¼€

        with open(json_file, 'r', encoding="utf8") as f:
            data = json.load(f)

        # æ„é€ è¾“å‡ºä¸­çš„ idã€imageã€depth å­—æ®µ
        new_id = f"{video_id}_{frame_id}"
        image_file = f"{video_id}{frame_id}_wide_image.png"
        depth_file = f"{video_id}{frame_id}_wide_depth.png"

        # æ„é€  conversationsï¼Œå¯¹æ¯ä¸ªåŸå¯¹è¯æ¡ç›®ç”Ÿæˆ human ä¸ gpt ä¸¤ä¸ªè½®æ¬¡
        convs = []
        for item in data.get("conversations", []):
            question = item.get("question", "").strip()
            answer = item.get("answer", "").strip()
            # å¦‚æœ question æˆ– answer å­˜åœ¨å†…å®¹åˆ™æ·»åŠ ï¼Œé¿å…ç©ºç™½å­—ç¬¦ä¸²
            if question:
                convs.append({"from": "human", "value": question})
            if answer:
                convs.append({"from": "gpt", "value": answer})

        new_item = {
            "id": new_id,
            "image": [image_file],
            "depth": [depth_file],
            "conversations": convs
        }
        # å¦‚æœ conversations ä¸ºç©ºï¼Œåˆ™è·³è¿‡
        if len(new_item["conversations"]) == 0:
            # print(f"âŒ ç›®å½• {frame_dir} æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ QA æ•°æ®ï¼Œè·³è¿‡")
            return None
        return new_item
    except Exception as e:
        print(f"âš ï¸ Error processing {json_file}: {e}")
        return None

if __name__ == "__main__":
    # è·å–æ‰€æœ‰ frame_json æ–‡ä»¶
    json_files = find_all_frame_json_files(ROOT_DIR)

    results = []
    # åˆ©ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†ï¼Œåˆ©ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
    with Pool(cpu_count()) as pool:
        for item in tqdm(pool.imap(process_frame_json, json_files), total=len(json_files)):
            if item is not None:
                results.append(item)

    # ä¿å­˜åˆå¹¶åçš„ç»“æœåˆ° ca1m_reasoning_qa.json æ–‡ä»¶
    output_path = Path(ROOT_DIR).parent / "ca1m_reasoning_qa.json"
    with open(output_path, "w", encoding="utf8") as f_out:
        json.dump(results, f_out, indent=2, ensure_ascii=False)

    print(f"ğŸ‰ åˆæˆå¤§ metadata JSON å®Œæˆï¼Œä¿å­˜è·¯å¾„ï¼š{output_path}")